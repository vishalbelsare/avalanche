:mod:`avalanche.training.strategies.base_strategy`
==================================================

.. py:module:: avalanche.training.strategies.base_strategy


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.strategies.base_strategy.BaseStrategy



.. py:class:: BaseStrategy(model: Module, optimizer: Optimizer, criterion, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = 1, device='cpu', plugins: Optional[Sequence['StrategyPlugin']] = None, evaluator=default_logger, eval_every=-1)

   BaseStrategy is the super class of all task-based continual learning
   strategies. It implements a basic training loop and callback system
   that allows to execute code at each experience of the training loop.
   Plugins can be used to implement callbacks to augment the training
   loop with additional behavior (e.g. a memory buffer for replay).

   **Scenarios**
   This strategy supports several continual learning scenarios:

   * class-incremental scenarios (no task labels)
   * multi-task scenarios, where task labels are provided)
   * multi-incremental scenarios, where the same task may be revisited

   The exact scenario depends on the data stream and whether it provides
   the task labels.

   **Training loop**
   The training loop is organized as follows::
       train
           train_exp  # for each experience
               adapt_train_dataset
               train_dataset_adaptation
               make_train_dataloader
               train_epoch  # for each epoch
                   # forward
                   # backward
                   # model update

   **Evaluation loop**
   The evaluation loop is organized as follows::
       eval
           eval_exp  # for each experience
               adapt_eval_dataset
               eval_dataset_adaptation
               make_eval_dataloader
               eval_epoch  # for each epoch
                   # forward
                   # backward
                   # model update

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param eval_mb_size: mini-batch size for eval.
   :param device: PyTorch device where the model will be allocated.
   :param plugins: (optional) list of StrategyPlugins.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations. None to remove logging.
   :param eval_every: the frequency of the calls to `eval` inside the
       training loop.
           if -1: no evaluation during training.
           if  0: calls `eval` after the final epoch of each training
               experience.
           if >0: calls `eval` every `eval_every` epochs and at the end
               of all the epochs for a single experience.

   .. attribute:: model
      :annotation: :Module

      PyTorch model. 


   .. attribute:: criterion
      

      Loss function. 


   .. attribute:: optimizer
      

      PyTorch optimizer. 


   .. attribute:: train_epochs
      :annotation: :int

      Number of training epochs. 


   .. attribute:: train_mb_size
      :annotation: :int

      Training mini-batch size. 


   .. attribute:: eval_mb_size
      :annotation: :int

      Eval mini-batch size. 


   .. attribute:: device
      

      PyTorch device where the model will be allocated. 


   .. attribute:: plugins
      

      List of `StrategyPlugin`s. 


   .. attribute:: evaluator
      

      EvaluationPlugin used for logging and metric computations. 


   .. attribute:: eval_every
      

      Frequency of the evaluation during training. 


   .. attribute:: training_exp_counter
      :annotation: = 0

      Counts the number of training steps. +1 at the end of each 
      experience. 


   .. attribute:: epoch
      :annotation: :Optional[int]

      Epoch counter. 


   .. attribute:: experience
      

      Current experience. 


   .. attribute:: adapted_dataset
      

      Data used to train. It may be modified by plugins. Plugins can 
      append data to it (e.g. for replay). 
       
      .. note:: 
          This dataset may contain samples from different experiences. If you 
          want the original data for the current experience  
          use :attr:`.BaseStrategy.experience`.


   .. attribute:: dataloader
      

      Dataloader. 


   .. attribute:: mb_it
      

      Iteration counter. Reset at the start of a new epoch. 


   .. attribute:: mbatch
      

      Current mini-batch. 


   .. attribute:: mb_x
      

      Current mini-batch input. 


   .. attribute:: mb_y
      

      Current mini-batch target. 


   .. attribute:: loss
      

      Loss of the current mini-batch. 


   .. attribute:: logits
      

      Logits computed on the current mini-batch. 


   .. attribute:: is_training
      :annotation: :bool = False

      True if the strategy is in training mode. 


   .. method:: is_eval(self)
      :property:

      True if the strategy is in evaluation mode. 


   .. method:: train(self, experiences: Union[Experience, Sequence[Experience]], eval_streams: Optional[Sequence[Union[Experience, Sequence[Experience]]]] = None, **kwargs)

      Training loop. if experiences is a single element trains on it.
      If it is a sequence, trains the model on each experience in order.
      This is different from joint training on the entire stream.
      It returns a dictionary with last recorded value for each metric.

      :param experiences: single Experience or sequence.
      :param eval_streams: list of streams for evaluation.
          If None: use training experiences for evaluation.
          Use [] if you do not want to evaluate during training.

      :return: dictionary containing last recorded value for
          each metric name.


   .. method:: train_exp(self, experience: Experience, eval_streams, **kwargs)

      Training loop over a single Experience object.

      :param experience: CL experience information.
      :param eval_streams: list of streams for evaluation.
          If None: use training experiences for evaluation.
          Use [] if you do not want to evaluate during training.
      :param kwargs: custom arguments.


   .. method:: train_dataset_adaptation(self, **kwargs)

      Initialize `self.adapted_dataset`. 


   .. method:: eval(self, exp_list: Union[Experience, Sequence[Experience]], **kwargs)

      Evaluate the current model on a series of experiences and
      returns the last recorded value for each metric.

      :param exp_list: CL experience information.
      :param kwargs: custom arguments.

      :return: dictionary containing last recorded value for
          each metric name


   .. method:: before_training_exp(self, **kwargs)

      Called  after the dataset and data loader creation and
      before the training loop.


   .. method:: make_train_dataloader(self, num_workers=0, shuffle=True, pin_memory=True, **kwargs)

      Called after the dataset adaptation. Initializes the data loader.
      :param num_workers: number of thread workers for the data loading.
      :param shuffle: True if the data should be shuffled, False otherwise.
      :param pin_memory: If True, the data loader will copy Tensors into CUDA
          pinned memory before returning them. Defaults to True.


   .. method:: make_eval_dataloader(self, num_workers=0, pin_memory=True, **kwargs)

      Initializes the eval data loader.
      :param num_workers: How many subprocesses to use for data loading.
          0 means that the data will be loaded in the main process.
          (default: 0).
      :param pin_memory: If True, the data loader will copy Tensors into CUDA
          pinned memory before returning them. Defaults to True.
      :param kwargs:
      :return:


   .. method:: after_train_dataset_adaptation(self, **kwargs)

      Called after the dataset adaptation and before the
      dataloader initialization. Allows to customize the dataset.
      :param kwargs:
      :return:


   .. method:: before_training_epoch(self, **kwargs)

      Called at the beginning of a new training epoch.
      :param kwargs:
      :return:


   .. method:: training_epoch(self, **kwargs)

      Training epoch.
      :param kwargs:
      :return:


   .. method:: before_training(self, **kwargs)


   .. method:: after_training(self, **kwargs)


   .. method:: before_training_iteration(self, **kwargs)


   .. method:: before_forward(self, **kwargs)


   .. method:: after_forward(self, **kwargs)


   .. method:: before_backward(self, **kwargs)


   .. method:: after_backward(self, **kwargs)


   .. method:: after_training_iteration(self, **kwargs)


   .. method:: before_update(self, **kwargs)


   .. method:: after_update(self, **kwargs)


   .. method:: after_training_epoch(self, **kwargs)


   .. method:: after_training_exp(self, **kwargs)


   .. method:: before_eval(self, **kwargs)


   .. method:: before_eval_exp(self, **kwargs)


   .. method:: eval_dataset_adaptation(self, **kwargs)

      Initialize `self.adapted_dataset`. 


   .. method:: before_eval_dataset_adaptation(self, **kwargs)


   .. method:: after_eval_dataset_adaptation(self, **kwargs)


   .. method:: eval_epoch(self, **kwargs)


   .. method:: after_eval_exp(self, **kwargs)


   .. method:: after_eval(self, **kwargs)


   .. method:: before_eval_iteration(self, **kwargs)


   .. method:: before_eval_forward(self, **kwargs)


   .. method:: after_eval_forward(self, **kwargs)


   .. method:: after_eval_iteration(self, **kwargs)


   .. method:: before_train_dataset_adaptation(self, **kwargs)


   .. method:: model_adaptation(self)


   .. method:: forward(self)


   .. method:: make_optimizer(self)



