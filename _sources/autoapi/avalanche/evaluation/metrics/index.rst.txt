:mod:`avalanche.evaluation.metrics`
===================================

.. py:module:: avalanche.evaluation.metrics

.. autoapi-nested-parse::

   The :py:mod:`metrics` module provides a set of already
   implemented metrics, ready to be used both standalone
   and together with the `EvaluationPlugin`.
   To use a standalone metric, please use the class which
   inherits from `Metric` and manually call the appropriate
   `update`, `reset` and 'result` method.
   To automatically monitor metrics during training and evaluation
   flows, specific classes which inherit from `PluginMetric`
   are provided. Most of these metrics can be created by leveraging
   the related helper function. Such function instantiates the same
   metric monitored on multiple callbacks (after each epoch, minibatch
   or experience). For example, to print accuracy metrics at the
   end of each training epoch and at the end of each evaluation experience,
   it is only required to call `accuracy_metrics(epoch=True, experience=True)`
   when creating the `EvaluationPlugin`.

   When available, please always use helper functions to specify
   the metrics to be monitored.



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   accuracy/index.rst
   confusion_matrix/index.rst
   cpu_usage/index.rst
   disk_usage/index.rst
   forgetting/index.rst
   gpu_usage/index.rst
   loss/index.rst
   mac/index.rst
   mean/index.rst
   ram_usage/index.rst
   timing/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.Mean
   avalanche.evaluation.metrics.Sum
   avalanche.evaluation.metrics.Accuracy
   avalanche.evaluation.metrics.MinibatchAccuracy
   avalanche.evaluation.metrics.EpochAccuracy
   avalanche.evaluation.metrics.RunningEpochAccuracy
   avalanche.evaluation.metrics.ExperienceAccuracy
   avalanche.evaluation.metrics.StreamAccuracy
   avalanche.evaluation.metrics.ConfusionMatrix
   avalanche.evaluation.metrics.StreamConfusionMatrix
   avalanche.evaluation.metrics.CPUUsage
   avalanche.evaluation.metrics.MinibatchCPUUsage
   avalanche.evaluation.metrics.EpochCPUUsage
   avalanche.evaluation.metrics.RunningEpochCPUUsage
   avalanche.evaluation.metrics.ExperienceCPUUsage
   avalanche.evaluation.metrics.StreamCPUUsage
   avalanche.evaluation.metrics.DiskUsage
   avalanche.evaluation.metrics.MinibatchDiskUsage
   avalanche.evaluation.metrics.EpochDiskUsage
   avalanche.evaluation.metrics.ExperienceDiskUsage
   avalanche.evaluation.metrics.StreamDiskUsage
   avalanche.evaluation.metrics.Forgetting
   avalanche.evaluation.metrics.ExperienceForgetting
   avalanche.evaluation.metrics.MaxGPU
   avalanche.evaluation.metrics.MinibatchMaxGPU
   avalanche.evaluation.metrics.EpochMaxGPU
   avalanche.evaluation.metrics.ExperienceMaxGPU
   avalanche.evaluation.metrics.StreamMaxGPU
   avalanche.evaluation.metrics.Loss
   avalanche.evaluation.metrics.MinibatchLoss
   avalanche.evaluation.metrics.EpochLoss
   avalanche.evaluation.metrics.RunningEpochLoss
   avalanche.evaluation.metrics.ExperienceLoss
   avalanche.evaluation.metrics.StreamLoss
   avalanche.evaluation.metrics.MAC
   avalanche.evaluation.metrics.MinibatchMAC
   avalanche.evaluation.metrics.EpochMAC
   avalanche.evaluation.metrics.ExperienceMAC
   avalanche.evaluation.metrics.MaxRAM
   avalanche.evaluation.metrics.MinibatchMaxRAM
   avalanche.evaluation.metrics.EpochMaxRAM
   avalanche.evaluation.metrics.ExperienceMaxRAM
   avalanche.evaluation.metrics.StreamMaxRAM
   avalanche.evaluation.metrics.ElapsedTime
   avalanche.evaluation.metrics.MinibatchTime
   avalanche.evaluation.metrics.EpochTime
   avalanche.evaluation.metrics.RunningEpochTime
   avalanche.evaluation.metrics.ExperienceTime
   avalanche.evaluation.metrics.StreamTime



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.accuracy_metrics
   avalanche.evaluation.metrics.cpu_usage_metrics
   avalanche.evaluation.metrics.disk_usage_metrics
   avalanche.evaluation.metrics.gpu_usage_metrics
   avalanche.evaluation.metrics.loss_metrics
   avalanche.evaluation.metrics.MAC_metrics
   avalanche.evaluation.metrics.ram_usage_metrics
   avalanche.evaluation.metrics.timing_metrics


.. py:class:: Mean

   Bases: :class:`Metric[float]`

   The standalone mean metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the mean of a sequence of values.

   Creates an instance of the mean metric.

   This metric in its initial state will return a mean value of 0.
   The metric can be updated by using the `update` method while the mean
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None

      Update the running mean given the value.

      The value can be weighted with a custom value, defined by the `weight`
      parameter.

      :param value: The value to be used to update the mean.
      :param weight: The weight of the value. Defaults to 1.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the mean.

      Calling this method will not change the internal state of the metric.

      :return: The mean, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Sum

   Bases: :class:`Metric[float]`

   The standalone sum metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the sum of a sequence of values.

   Beware that this metric only supports summing numbers and the result is
   always a float value, even when `update` is called by passing `int`s only.

   Creates an instance of the sum metric.

   This metric in its initial state will return a sum value of 0.
   The metric can be updated by using the `update` method while the sum
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat) -> None

      Update the running sum given the value.

      :param value: The value to be used to update the sum.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the sum.

      Calling this method will not change the internal state of the metric.

      :return: The sum, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Accuracy

   Bases: :class:`Metric[float]`

   The Accuracy metric. This is a standalone metric
   used to compute more specific ones.

   Instances of this metric keeps the running average accuracy
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average accuracy
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an accuracy value of 0.

   Creates an instance of the standalone Accuracy metric.

   By default this metric in its initial state will return an accuracy
   value of 0. The metric can be updated by using the `update` method
   while the running accuracy can be retrieved using the `result` method.

   .. attribute:: _mean_accuracy
      

      The mean utility that will be used to store the running accuracy.


   .. method:: update(self, predicted_y: Tensor, true_y: Tensor) -> None

      Update the running accuracy given the true and predicted labels.

      :param predicted_y: The model prediction. Both labels and logit vectors
          are supported.
      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running accuracy.

      Calling this method will not change the internal state of the metric.

      :return: The running accuracy, as a float value between 0 and 1.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchAccuracy

   Bases: :class:`PluginMetric[float]`

   The minibatch plugin accuracy metric.
   This metric only works at training time.

   This metric computes the average accuracy over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochAccuracy` instead.

   Creates an instance of the MinibatchAccuracy metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochAccuracy

   Bases: :class:`PluginMetric[float]`

   The average accuracy over a single training epoch.
   This plugin metric only works at training time.

   The accuracy will be logged after each training epoch by computing
   the number of correctly predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochAccuracy metric.

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> None


   .. method:: before_training_epoch(self, strategy: BaseStrategy) -> None


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.EpochAccuracy`

   The average accuracy across all minibatches up to the current
   epoch iteration.
   This plugin metric only works at training time.

   At each iteration, this metric logs the accuracy averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochAccuracy metric.

   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> None


   .. method:: _package_result(self, strategy: BaseStrategy)


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceAccuracy

   Bases: :class:`PluginMetric[float]`

   At the end of each experience, this plugin metric reports
   the average accuracy over all patterns seen in that experience.
   This metric only works at eval time.

   Creates an instance of ExperienceAccuracy metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamAccuracy

   Bases: :class:`PluginMetric[float]`

   At the end of the entire stream of experiences, this plugin metric
   reports the average accuracy over all patterns seen in all experiences.
   This metric only works at eval time.

   Creates an instance of StreamAccuracy metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_eval(self, strategy: BaseStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: accuracy_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the minibatch accuracy at training time.
   :param epoch: If True, will return a metric able to log
       the epoch accuracy at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch accuracy at training time.
   :param experience: If True, will return a metric able to log
       the accuracy on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the accuracy averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


.. py:class:: ConfusionMatrix(num_classes: int = None)

   Bases: :class:`Metric[Tensor]`

   The standalone confusion matrix metric.

   Instances of this metric keep track of the confusion matrix by receiving a
   pair of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the unnormalized running confusion matrix.

   Beware that by default the confusion matrix size will depend on the value of
   the maximum label as detected by looking at both the ground truth and
   predictions Tensors. When passing one-hot/logit vectors, this
   metric will try to infer the number of classes from the vector sizes.
   Otherwise, the maximum label value encountered in the truth/prediction
   Tensors will be used. It is recommended to set the (initial) number of
   classes in the constructor.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an empty Tensor.

   Creates an instance of the standalone confusion matrix metric.

   By default this metric in its initial state will return an empty Tensor.
   The metric can be updated by using the `update` method while the running
   confusion matrix can be retrieved using the `result` method.

   :param num_classes: The initial number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details).

   .. attribute:: _cm_tensor
      :annotation: :Optional[Tensor]

      The Tensor where the running confusion matrix is stored.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running confusion matrix given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> Tensor

      Retrieves the unnormalized confusion matrix.

      Calling this method will not change the internal state of the metric.

      :return: The running confusion matrix, as a Tensor.


   .. method:: reset(self) -> None

      Resets the metric.

      Calling this method will *not* reset the default number of classes
      optionally defined in the constructor optional parameter.

      :return: None.



.. py:class:: StreamConfusionMatrix(*, num_classes: Union[int, Mapping[int, int]] = None, normalize: Literal['true', 'pred', 'all'] = None, save_image: bool = True, image_creator: Callable[[Tensor], Image] = default_cm_image_creator)

   Bases: :class:`PluginMetric[Tensor]`

   The Stream Confusion Matrix metric.
   This plugin metric only works on the eval phase.

   At the end of the eval phase, this metric logs the confusion matrix
   relative to all the patterns seen during eval.

   The metric can log either a Tensor or a PIL Image representing the
   confusion matrix.

   Creates an instance of the Stream Confusion Matrix metric.

   :param num_classes: When not None, is used to properly define the
       amount of rows/columns in the confusion matrix. When None, the
       matrix will have many rows/columns as the maximum value of the
       predicted and true pattern labels. Can be either an int, in which
       case the same value will be used across all experiences, or a
       dictionary defining the amount of classes for each experience (key =
        experience label, value = amount of classes). Defaults to None.
   :param normalize: Normalizes confusion matrix over the true (rows),
       predicted (columns) conditions or all the population. If None,
       confusion matrix will not be normalized. Valid values are: 'true',
       'pred' and 'all' or None.
   :param save_image: If True, a graphical representation of the confusion
       matrix will be logged, too. If False, only the Tensor representation
       will be logged. Defaults to True.
   :param image_creator: A callable that, given the tensor representation
       of the confusion matrix, returns a graphical representation of the
       matrix as a PIL Image. Defaults to `default_cm_image_creator`.

   .. method:: reset(self) -> None


   .. method:: result(self) -> Tensor


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None


   .. method:: before_eval(self, strategy) -> None


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _class_num_for_exp(self, exp_label: int) -> Optional[int]


   .. method:: _normalize_cm(cm: Tensor, normalization: Literal['true', 'pred', 'all'])
      :staticmethod:


   .. method:: nan_to_num(matrix: Tensor) -> Tensor
      :staticmethod:


   .. method:: __str__(self)

      Return str(self).



.. py:class:: CPUUsage

   Bases: :class:`Metric[float]`

   The standalone CPU usage metric.

   Instances of this metric compute the average CPU usage as a float value.
   The metric starts tracking the CPU usage when the `update` method is called
   for the first time. That is, the tracking does not start at the time the
   constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   average usage between the first and the last call to `update`.

   The result, obtained using the `result` method, is the usage computed
   as stated above.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the standalone CPU usage metric.

   By default this metric in its initial state will return a CPU usage
   value of 0. The metric can be updated by using the `update` method
   while the average CPU usage can be retrieved using the `result` method.

   .. attribute:: _mean_usage
      

      The mean utility that will be used to store the average usage.


   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: _first_update
      :annotation: = True

      An internal flag to keep track of the first call to the `update` method.


   .. method:: update(self) -> None

      Update the running CPU usage.

      For more info on how to set the starting moment see the class
      description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the average CPU usage.

      Calling this method will not change the internal state of the metric.

      :return: The average CPU usage, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchCPUUsage

   Bases: :class:`PluginMetric[float]`

   The minibatch CPU usage metric.
   This plugin metric only works at training time.

   This metric "logs" the CPU usage for each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochCPUUsage`.

   Creates an instance of the minibatch CPU usage metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochCPUUsage

   Bases: :class:`PluginMetric[float]`

   The Epoch CPU usage metric.
   This plugin metric only works at training time.

   The average usage will be logged after each epoch.

   Creates an instance of the epoch CPU usage metric.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochCPUUsage

   Bases: :class:`PluginMetric[float]`

   The running epoch CPU usage metric.
   This plugin metric only works at training time

   After each iteration, the metric logs the average CPU usage up
   to the current epoch iteration.

   Creates an instance of the average epoch cpu usage metric.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_training_iteration(self, strategy: BaseStrategy) -> 'MetricResult'


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceCPUUsage

   Bases: :class:`PluginMetric[float]`

   The average experience CPU usage metric.
   This plugin metric works only at eval time.

   After each experience, this metric emits the average CPU usage on that
   experienc.

   Creates an instance of the experience CPU usage metric.

   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamCPUUsage

   Bases: :class:`PluginMetric[float]`

   The average stream CPU usage metric.
   This plugin metric works only at eval time.

   After the entire evaluation stream, this metric emits
   the average CPU usage on all experiences.

   Creates an instance of the stream CPU usage metric.

   .. method:: before_eval(self, strategy: BaseStrategy) -> None


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: cpu_usage_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log the minibatch
       CPU usage
   :param epoch: If True, will return a metric able to log the epoch
       CPU usage
   :param epoch_running: If True, will return a metric able to log the running
       epoch CPU usage.
   :param experience: If True, will return a metric able to log the experience
       CPU usage.
   :param stream: If True, will return a metric able to log the evaluation
       stream CPU usage.

   :return: A list of plugin metrics.


.. py:class:: DiskUsage(paths_to_monitor: Union[PathAlike, Sequence[PathAlike]] = None)

   Bases: :class:`Metric[float]`

   The standalone disk usage metric.

   This metric can be used to monitor the size of a set of directories.
   e.g. This can be useful to monitor the size of a replay buffer,

   Creates an instance of the standalone disk usage metric.

   The `result` method will return the sum of the size
   of the directories specified as the first parameter in KiloBytes.

   :param paths_to_monitor: a path or a list of paths to monitor. If None,
       the current working directory is used. Defaults to None.

   .. method:: update(self)

      Updates the disk usage statistics.

      :return None.


   .. method:: result(self) -> Optional[float]

      Retrieves the disk usage as computed during the last call to the
      `update` method.

      Calling this method will not change the internal state of the metric.

      :return: The disk usage or None if `update` was not invoked yet.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: get_dir_size(path: str)
      :staticmethod:



.. py:class:: MinibatchDiskUsage(paths_to_monitor)

   Bases: :class:`PluginMetric[float]`

   The minibatch Disk usage metric.
   This plugin metric only works at training time.

   At the end of each iteration, this metric logs the total
   size (in KB) of all the monitored paths.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochDiskUsage`.

   Creates an instance of the minibatch Disk usage metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochDiskUsage(paths_to_monitor)

   Bases: :class:`PluginMetric[float]`

   The Epoch Disk usage metric.
   This plugin metric only works at training time.

   At the end of each epoch, this metric logs the total
   size (in KB) of all the monitored paths.

   Creates an instance of the epoch Disk usage metric.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceDiskUsage(paths_to_monitor)

   Bases: :class:`PluginMetric[float]`

   The average experience Disk usage metric.
   This plugin metric works only at eval time.

   At the end of each experience, this metric logs the total
   size (in KB) of all the monitored paths.

   Creates an instance of the experience Disk usage metric.

   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamDiskUsage(paths_to_monitor)

   Bases: :class:`PluginMetric[float]`

   The average stream Disk usage metric.
   This plugin metric works only at eval time.

   At the end of the eval stream, this metric logs the total
   size (in KB) of all the monitored paths.

   Creates an instance of the stream Disk usage metric.

   .. method:: before_eval(self, strategy: BaseStrategy) -> None


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: disk_usage_metrics(*, paths_to_monitor=None, minibatch=False, epoch=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   standalone metrics.

   :param minibatch: If True, will return a metric able to log the minibatch
       Disk usage
   :param epoch: If True, will return a metric able to log the epoch
       Disk usage
   :param experience: If True, will return a metric able to log the experience
       Disk usage.
   :param stream: If True, will return a metric able to log the evaluation
       stream Disk usage.

   :return: A list of plugin metrics.


.. py:class:: Forgetting

   Bases: :class:`Metric[Union[float, None, Dict[int, float]]]`

   The standalone Forgetting metric.
   This metric returns the forgetting relative to a specific key.
   Alternatively, this metric returns a dict in which each key is associated
   to the forgetting.
   Forgetting is computed as the difference between the first value recorded
   for a specific key and the last value recorded for that key.
   The value associated to a key can be update with the `update` method.

   At initialization, this metric returns an empty dictionary.

   Creates an instance of the standalone Forgetting metric

   .. attribute:: initial
      :annotation: :Dict[int, float]

      The initial value for each key.


   .. attribute:: last
      :annotation: :Dict[int, float]

      The last value detected for each key


   .. method:: update_initial(self, k, v)


   .. method:: update_last(self, k, v)


   .. method:: update(self, k, v, initial=False)


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      Forgetting is returned only for keys encountered twice.

      :param k: the key for which returning forgetting. If k has not
          updated at least twice it returns None. If k is None,
          forgetting will be returned for all keys encountered at least
          twice.

      :return: the difference between the first and last value encountered
          for k, if k is not None. It returns None if k has not been updated
          at least twice. If k is None, returns a dictionary
          containing keys whose value has been updated at least twice. The
          associated value is the difference between the first and last
          value recorded for that key.


   .. method:: reset_last(self) -> None


   .. method:: reset(self) -> None

      Resets the metric internal state.

      :return: None.



.. py:class:: ExperienceForgetting

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The ExperienceForgetting metric, describing the accuracy loss
   detected for a certain experience.

   This plugin metric, computed separately for each experience,
   is the difference between the accuracy result obtained after
   first training on a experience and the accuracy result obtained
   on the same experience at the end of successive experiences.

   This metric is computed during the eval phase only.

   Creates an instance of the ExperienceForgetting metric.

   .. attribute:: forgetting
      

      The general metric to compute forgetting


   .. attribute:: _last_accuracy
      

      The average accuracy over the current evaluation experience


   .. attribute:: eval_exp_id
      

      The current evaluation experience id


   .. attribute:: train_exp_id
      

      The last encountered training experience id


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each
      experience!

      :return: None.


   .. method:: reset_last_accuracy(self) -> None

      Resets the last accuracy.

      This will preserve the initial accuracy value of each experience.
      To be used at the beginning of each eval experience.

      :return: None.


   .. method:: update(self, k, v, initial=False)

      Update forgetting metric.
      See `Forgetting` for more detailed information.

      :param k: key to update
      :param v: value associated to k
      :param initial: update initial value. If False, update
          last value.


   .. method:: result(self, k=None) -> Union[float, None, Dict[int, float]]

      See `Forgetting` documentation for more detailed information.

      k: optional key from which compute forgetting.


   .. method:: before_training_exp(self, strategy: BaseStrategy) -> None


   .. method:: before_eval(self, strategy) -> None


   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: MaxGPU(gpu_id, every=0.5)

   Bases: :class:`Metric[float]`

   The standalone GPU usage metric.
   Important: this metric approximates the real maximum GPU percentage
    usage since it sample at discrete amount of time the GPU values.

   Instances of this metric keeps the maximum GPU usage percentage detected.
   The `start_thread` method starts the usage tracking.
   The `stop_thread` method stops the tracking.

   The result, obtained using the `result` method, is the usage in mega-bytes.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the GPU usage metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. attribute:: thread
      

      Thread executing GPU monitoring code


   .. attribute:: stop_f
      :annotation: = False

      Flag to stop the thread


   .. attribute:: max_usage
      :annotation: = 0

      Main metric result. Max GPU usage.


   .. method:: _f(self)

      Until a stop signal is encountered,
      this function monitors each `every` seconds
      the maximum amount of GPU used by the process


   .. method:: start_thread(self)


   .. method:: stop_thread(self)


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Returns the max GPU percentage value.

      :return: The percentage GPU usage as a float value in range [0, 1].



.. py:class:: MinibatchMaxGPU(gpu_id, every=0.5)

   Bases: :class:`PluginMetric[float]`

   The Minibatch Max GPU metric.
   This plugin metric only works at training time.

   Creates an instance of the Minibatch Max GPU metric

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_training(self, strategy: BaseStrategy) -> None


   .. method:: before_training_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochMaxGPU(gpu_id, every=0.5)

   Bases: :class:`PluginMetric[float]`

   The Epoch Max GPU metric.
   This plugin metric only works at training time.

   Creates an instance of the epoch Max GPU metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_training(self, strategy: BaseStrategy) -> None


   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceMaxGPU(gpu_id, every=0.5)

   Bases: :class:`PluginMetric[float]`

   The Experience Max GPU metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_eval(self, strategy: BaseStrategy) -> None


   .. method:: before_eval_exp(self, strategy) -> MetricResult


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_eval(self, strategy: BaseStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamMaxGPU(gpu_id, every=0.5)

   Bases: :class:`PluginMetric[float]`

   The Stream Max GPU metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage

   .. method:: before_eval(self, strategy) -> MetricResult


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: gpu_usage_metrics(gpu_id, every=0.5, minibatch=False, epoch=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param gpu_id: GPU device ID.
   :param every: seconds after which update the maximum GPU
       usage
   :param minibatch: If True, will return a metric able to log the minibatch
       max GPU usage.
   :param epoch: If True, will return a metric able to log the epoch
       max GPU usage.
   :param experience: If True, will return a metric able to log the experience
       max GPU usage.
   :param stream: If True, will return a metric able to log the evaluation
       max stream GPU usage.

   :return: A list of plugin metrics.


.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The standalone Loss metric. This is a general metric
   used to compute more specific ones.

   Instances of this metric keeps the running average loss
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average loss
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return a loss value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.

      :return: The running loss, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchLoss

   Bases: :class:`PluginMetric[float]`

   The minibatch loss metric.
   This plugin metric only works at training time.

   This metric computes the average loss over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochLoss

   Bases: :class:`PluginMetric[float]`

   The average loss over a single training epoch.
   This plugin metric only works at training time.

   The loss will be logged after each training epoch by computing
   the loss on the predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochLoss metric.

   .. method:: before_training_epoch(self, strategy: BaseStrategy) -> None


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.EpochLoss`

   The average loss across all minibatches up to the current
   epoch iteration.
   This plugin metric only works at training time.

   At each iteration, this metric logs the loss averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochLoss metric.

   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> None


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceLoss

   Bases: :class:`PluginMetric[float]`

   At the end of each experience, this metric reports
   the average loss over all patterns seen in that experience.
   This plugin metric only works at eval time.

   Creates an instance of ExperienceLoss metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamLoss

   Bases: :class:`PluginMetric[float]`

   At the end of the entire stream of experiences, this metric reports the
   average loss over all patterns seen in all experiences.
   This plugin metric only works at eval time.

   Creates an instance of StreamLoss metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval(self, strategy: BaseStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_eval(self, strategy: BaseStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the minibatch loss at training time.
   :param epoch: If True, will return a metric able to log
       the epoch loss at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch loss at training time.
   :param experience: If True, will return a metric able to log
       the loss on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the loss averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


.. py:class:: MAC

   Bases: :class:`Metric[int]`

   Standalone Multiply-and-accumulate metric. Provides a lower bound of the
   computational cost of a model in a hardware-independent way by
   computing the number of multiplications. Currently supports only
   Linear or Conv2d modules. Other operations are ignored.

   Creates an instance of the MAC metric.

   .. method:: update(self, model: Module, dummy_input: Tensor)

      Computes the MAC metric.

      :param model: current model.
      :param dummy_input: A tensor of the correct size to feed as input
          to model. It includes batch size
      :return: MAC metric.


   .. method:: result(self) -> Optional[int]

      Return the number of MAC operations as computed in the previous call
      to the `update` method.

      :return: The number of MAC operations or None if `update` has not been
          called yet.


   .. method:: update_compute_cost(self, module, dummy_input, output)


   .. method:: is_recognized_module(mod)
      :staticmethod:



.. py:class:: MinibatchMAC

   Bases: :class:`PluginMetric[float]`

   The minibatch MAC metric.
   This plugin metric only works at training time.

   This metric computes the MAC over 1 pattern
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochMAC` instead.

   Creates an instance of the MinibatchMAC metric.

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochMAC

   Bases: :class:`PluginMetric[float]`

   The MAC at the end of each epoch computed on a
   single pattern.
   This plugin metric only works at training time.

   The MAC will be logged after each training epoch.

   Creates an instance of the EpochMAC metric.

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceMAC

   Bases: :class:`PluginMetric[float]`

   At the end of each experience, this metric reports the
   MAC computed on a single pattern.
   This plugin metric only works at eval time.

   Creates an instance of ExperienceMAC metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: MAC_metrics(*, minibatch=False, epoch=False, experience=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the MAC after each iteration at training time.
   :param epoch: If True, will return a metric able to log
       the MAC after each epoch at training time.
   :param experience: If True, will return a metric able to log
       the MAC after each eval experience.

   :return: A list of plugin metrics.


.. py:class:: MaxRAM(every=1)

   Bases: :class:`Metric[float]`

   The standalone RAM usage metric.
   Important: this metric approximates the real maximum RAM usage since
   it sample at discrete amount of time the RAM values.

   Instances of this metric keeps the maximum RAM usage detected.
   The `start_thread` method starts the usage tracking.
   The `stop_thread` method stops the tracking.

   The result, obtained using the `result` method, is the usage in mega-bytes.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the RAM usage metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: stop_f
      :annotation: = False

      Flag to stop the thread


   .. attribute:: max_usage
      :annotation: = 0

      Main metric result. Max RAM usage.


   .. attribute:: thread
      

      Thread executing RAM monitoring code


   .. method:: _f(self)

      Until a stop signal is encountered,
      this function monitors each `every` seconds
      the maximum amount of RAM used by the process


   .. method:: result(self) -> Optional[float]

      Retrieves the RAM usage.

      Calling this method will not change the internal state of the metric.

      :return: The average RAM usage in bytes, as a float value.


   .. method:: start_thread(self)


   .. method:: stop_thread(self)


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchMaxRAM(every=1)

   Bases: :class:`PluginMetric[float]`

   The Minibatch Max RAM metric.
   This plugin metric only works at training time.

   Creates an instance of the Minibatch Max RAM metric
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_training(self, strategy: BaseStrategy) -> None


   .. method:: before_training_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochMaxRAM(every=1)

   Bases: :class:`PluginMetric[float]`

   The Epoch Max RAM metric.
   This plugin metric only works at training time.

   Creates an instance of the epoch Max RAM metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_training(self, strategy: BaseStrategy) -> None


   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_training(self, strategy: BaseStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceMaxRAM(every=1)

   Bases: :class:`PluginMetric[float]`

   The Experience Max RAM metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_eval(self, strategy: BaseStrategy) -> None


   .. method:: before_eval_exp(self, strategy) -> MetricResult


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_eval(self, strategy: BaseStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamMaxRAM(every=1)

   Bases: :class:`PluginMetric[float]`

   The Stream Max RAM metric.
   This plugin metric only works at eval time.

   Creates an instance of the Experience CPU usage metric.
   :param every: seconds after which update the maximum RAM
       usage

   .. method:: before_eval(self, strategy) -> MetricResult


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: ram_usage_metrics(*, every=1, minibatch=False, epoch=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param every: seconds after which update the maximum RAM
       usage
   :param minibatch: If True, will return a metric able to log the minibatch
       max RAM usage.
   :param epoch: If True, will return a metric able to log the epoch
       max RAM usage.
   :param experience: If True, will return a metric able to log the experience
       max RAM usage.
   :param stream: If True, will return a metric able to log the evaluation
       max stream RAM usage.

   :return: A list of plugin metrics.


.. py:class:: ElapsedTime

   Bases: :class:`Metric[float]`

   The standalone Elapsed Time metric.

   Instances of this metric keep track of the time elapsed between calls to the
   `update` method. The starting time is set when the `update` method is called
   for the first time. That is, the starting time is *not* taken at the time
   the constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   elapsed time between the first and the last call to `update`.

   The result, obtained using the `result` method, is the time, in seconds,
   computed as stated above.

   The `reset` method will set the metric to its initial state, thus resetting
   the initial time. This metric in its initial state (or if the `update`
   method was invoked only once) will return an elapsed time of 0.

   Creates an instance of the ElapsedTime metric.

   This metric in its initial state (or if the `update` method was invoked
   only once) will return an elapsed time of 0. The metric can be updated
   by using the `update` method while the running accuracy can be retrieved
   using the `result` method.

   .. method:: update(self) -> None

      Update the elapsed time.

      For more info on how to set the initial time see the class description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the elapsed time.

      Calling this method will not change the internal state of the metric.

      :return: The elapsed time, in seconds, as a float value.


   .. method:: reset(self) -> None

      Resets the metric, including the initial time.

      :return: None.



.. py:class:: MinibatchTime

   Bases: :class:`PluginMetric[float]`

   The minibatch time metric.
   This plugin metric only works at training time.

   This metric "logs" the elapsed time for each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochTime`.

   Creates an instance of the minibatch time metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochTime

   Bases: :class:`PluginMetric[float]`

   The epoch elapsed time metric.
   This plugin metric only works at training time.

   The elapsed time will be logged after each epoch.

   Creates an instance of the epoch time metric.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochTime

   Bases: :class:`PluginMetric[float]`

   The running epoch time metric.
   This plugin metric only works at training time.

   For each iteration, this metric logs the average time
   between the start of the
   epoch and the current iteration.

   Creates an instance of the running epoch time metric..

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_training_iteration(self, strategy: BaseStrategy) -> None


   .. method:: after_training_iteration(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceTime

   Bases: :class:`PluginMetric[float]`

   The experience time metric.
   This plugin metric only works at eval time.

   After each experience, this metric emits the average time of that
   experience.

   Creates an instance of the experience time metric.

   .. method:: before_eval_exp(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_eval_exp(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamTime

   Bases: :class:`PluginMetric[float]`

   The stream time metric.
   This metric only works at eval time.

   After the entire evaluation stream,
   this plugin metric emits the average time of that stream.

   Creates an instance of the stream time metric.

   .. method:: before_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: after_eval(self, strategy: BaseStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: BaseStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: timing_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log the train
       minibatch elapsed time.
   :param epoch: If True, will return a metric able to log the train epoch
       elapsed time.
   :param epoch_running: If True, will return a metric able to log the running
       train epoch elapsed time.
   :param experience: If True, will return a metric able to log the eval
       experience elapsed time.
   :param stream: If True, will return a metric able to log the eval stream
       elapsed time.

   :return: A list of plugin metrics.


